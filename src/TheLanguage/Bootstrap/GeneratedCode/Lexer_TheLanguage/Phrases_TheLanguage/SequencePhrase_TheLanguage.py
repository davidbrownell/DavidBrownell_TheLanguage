# ----------------------------------------------------------------------
# |
# |  This file has been automatically generated by PythonVisitor.py.
# |
# ----------------------------------------------------------------------
"""\
Contains the `SequencePhrase` object.
"""


import copy
from enum import auto, Enum

from CommonEnvironmentEx.Package import InitRelativeImports

with InitRelativeImports():
    from ...CommonLibrary import HashLib_TheLanguage as HashLib
    from ...CommonLibrary.Int_TheLanguage import *
    from ...CommonLibrary.List_TheLanguage import List
    # from ...CommonLibrary.Num_TheLanguage import Num
    # from ...CommonLibrary.Queue_TheLanguage import Queue
    from ...CommonLibrary.Range_TheLanguage import *
    from ...CommonLibrary.Set_TheLanguage import Set
    from ...CommonLibrary.Stack_TheLanguage import Stack
    from ...CommonLibrary.String_TheLanguage import String

    from .RecursivePlaceholderPhrase_TheLanguage import RecursivePlaceholderPhrase
    from .TokenPhrase_TheLanguage import TokenPhrase
    from ..Components_TheLanguage.Phrase_TheLanguage import Phrase, NormalizedIterator
    from ..Components_TheLanguage.Token_TheLanguage import ControlTokenBase, DedentToken, HorizontalWhitespaceToken, IndentToken, NewlineToken, PopIgnoreWhitespaceControlToken, PopPreserveWhitespaceControlToken, PushIgnoreWhitespaceControlToken, PushPreserveWhitespaceControlToken, RegexToken

# Visibility: public
# ClassModifier: mutable
# ClassType: Class
class SequencePhrase(Phrase):
    """\
    Matches a sequence of `Phrases`.
    """

    def __init__(self, *args, **kwargs):
        SequencePhrase._InternalInit(self, list(args), kwargs)

    def _InternalInit(self, args, kwargs):
        # comment_token, _name_is_default, _phrases

        Phrase._InternalInit(self, args, kwargs)

        # comment_token
        if "comment_token" in kwargs:
            self.comment_token = kwargs.pop("comment_token")
        elif args:
            self.comment_token = args.pop(0)
        else:
            raise Exception("comment_token was not provided")

        # _name_is_default
        if "_name_is_default" in kwargs:
            self._name_is_default = kwargs.pop("_name_is_default")
        elif args:
            self._name_is_default = args.pop(0)
        else:
            raise Exception("_name_is_default was not provided")

        # _phrases
        if "_phrases" in kwargs:
            self._phrases = kwargs.pop("_phrases")
        elif args:
            self._phrases = args.pop(0)
        else:
            raise Exception("_phrases was not provided")

        # _indent_token
        self._indent_token = IndentToken.Create()

        # _dedent_token
        self._dedent_token = DedentToken.Create()

        # _horizontal_whitespace_token
        self._horizontal_whitespace_token = HorizontalWhitespaceToken.Create()

        # _newline_token
        self._newline_token = NewlineToken.Create()

        self._Init_36aebe90904f4bd0b82715aa47aa05b5_()

    def __eq__(self, other):
        if Phrase.__eq__(self, other) is False: return False
        if not isinstance(other, self.__class__): return False
        return self.__class__.__Compare__(self, other) == 0

    def __ne__(self, other):
        if Phrase.__ne__(self, other) is False: return False
        if not isinstance(other, self.__class__): return True
        return self.__class__.__Compare__(self, other) != 0

    def __lt__(self, other):
        if Phrase.__lt__(self, other) is False: return False
        if not isinstance(other, self.__class__): return False
        return self.__class__.__Compare__(self, other) < 0

    def __le__(self, other):
        if Phrase.__le__(self, other) is False: return False
        if not isinstance(other, self.__class__): return False
        return self.__class__.__Compare__(self, other) <= 0

    def __gt__(self, other):
        if Phrase.__gt__(self, other) is False: return False
        if not isinstance(other, self.__class__): return False
        return self.__class__.__Compare__(self, other) > 0

    def __ge__(self, other):
        if Phrase.__ge__(self, other) is False: return False
        if not isinstance(other, self.__class__): return False
        return self.__class__.__Compare__(self, other) >= 0

    @classmethod
    def __Compare__(cls, a, b):
        result = Phrase.__Compare__(a, b)
        if result != 0: return result

        result = cls.__CompareItem__(a.comment_token, b.comment_token)
        if result is not None: return result

        result = cls.__CompareItem__(a._name_is_default, b._name_is_default)
        if result is not None: return result

        result = cls.__CompareItem__(a._phrases, b._phrases)
        if result is not None: return result

        result = cls.__CompareItem__(a._indent_token, b._indent_token)
        if result is not None: return result

        result = cls.__CompareItem__(a._dedent_token, b._dedent_token)
        if result is not None: return result

        result = cls.__CompareItem__(a._horizontal_whitespace_token, b._horizontal_whitespace_token)
        if result is not None: return result

        result = cls.__CompareItem__(a._newline_token, b._newline_token)
        if result is not None: return result

        return 0

    @classmethod
    def __CompareItem__(cls, a, b):
        if a is None and b is None:
            return None

        if a is None: return -1
        if b is None: return 1

        try:
            if a < b: return -1
            if a > b: return 1
        except TypeError:
            a = id(a)
            b = id(b)

            if a < b: return -1
            if a > b: return 1

        return None

    # Type alias: public Phrases = List<Phrase, >{min_length'=1, }
    @property
    def Phrases(self): return self._phrases
    def __hash__(self): return super(SequencePhrase, self).__hash__()
    # Return Type: SequencePhrase
    @staticmethod
    def Create(regex_token, phrases, name=None, ):
        name_is_default = None
        if name is None:
            name = SequencePhrase._CreateDefaultName(phrases, )
            name_is_default = True
        else:
            name_is_default = False

        return SequencePhrase(name, regex_token, name_is_default, phrases, )

    # Return Type: <LexResult | None> val
    def Lex(self, unique_id, iter, observer, ignore_whitespace=False, ):
        NormalizedIteratorRange = Phrase.NormalizedIteratorRange
        result = None # as <LexResult | None>
        observer.StartPhrase(unique_id, self, )
        ignored_indentation_level = None
        if self._phrases[0].__class__.__name__ == "TokenPhrase" and self._phrases[0].token.__class__.__name__ == "PushIgnoreWhitespaceControlToken":
            ignored_indentation_level = 0
        else:
            ignored_indentation_level = None

        result = self._LexImpl(unique_id, iter, observer, ignore_whitespace_ctr=1 if ignore_whitespace else 0, ignored_indentation_level=ignored_indentation_level, starting_phrase_index=0, )
        if result is None:
            return None

        if result.success and not observer.OnInternalPhraseProxy(result.data, result.range, ):
            return None

        return result

    # Return Type: <LexResult | None> val
    def LexSuffix(self, unique_id, iter, observer, ignore_whitespace=False, ):
        return self._LexImpl(unique_id, iter, observer, ignore_whitespace_ctr=1 if ignore_whitespace else 0, ignored_indentation_level=None, starting_phrase_index=1, )

    # Return Type: None
    def _Init_36aebe90904f4bd0b82715aa47aa05b5_(self):
        pass

    # Return Type: Bool val
    def _PopulateRecursiveImpl(self, new_phrase, ):
        replaced_phrase = False
        for (phrase_index, phrase, ) in Enumerate(self._phrases, ):
            if phrase.__class__.__name__ == "RecursivePlaceholderPhrase":
                self._phrases[phrase_index] = new_phrase
                replaced_phrase = True
            else:
                replaced_phrase = phrase.PopulateRecursive(self, new_phrase, ) or replaced_phrase

        if replaced_phrase and self._name_is_default:
            self._name_ = SequencePhrase._CreateDefaultName(self._phrases, )

        return replaced_phrase

    # Return Type: <LexResult | None> val
    def _LexImpl(self, unique_id, iter, observer, ignore_whitespace_ctr, ignored_indentation_level, starting_phrase_index, ):
        LexResult = Phrase.LexResult
        TokenLexResultData = Phrase.TokenLexResultData
        WhitespaceLexResultData = Phrase.WhitespaceLexResultData
        NormalizedIteratorRange = Phrase.NormalizedIteratorRange
        PhraseLexResultData = Phrase.PhraseLexResultData
        original_iter = iter.Clone()
        # Return Type: <List<LexResultData, >{min_length'=1, } | None> val
        def ExtractIgnoredDataItems():
            nonlocal self
            nonlocal iter
            nonlocal ignore_whitespace_ctr
            nonlocal ignored_indentation_level

            at_content_start = iter.OffsetProper() == 0
            data_items = List()
            eat_next_newline = False
            while not iter.AtEnd():
                next_token_type = iter.GetNextTokenType()
                if next_token_type == NormalizedIterator.TokenType.Indent:
                    if ignore_whitespace_ctr == 0 and ignored_indentation_level is None:
                        break

                    result = self._indent_token.Match_(iter.Clone(), )
                    assert result is not None
                    if ignored_indentation_level is None:
                        data_items.InsertBack_(TokenLexResultData(self._indent_token, result, NormalizedIteratorRange(iter, result.iterator, ), is_ignored=True, ), )
                    else:
                        ignored_indentation_level += 1

                    iter = result.iterator.Clone()
                elif next_token_type == NormalizedIterator.TokenType.Dedent:
                    if ignore_whitespace_ctr == 0 and ignored_indentation_level is None:
                        break

                    result = self._dedent_token.Match_(iter.Clone(), )
                    assert result is not None
                    if ignored_indentation_level is None:
                        data_items.InsertBack_(TokenLexResultData(self._dedent_token, result, NormalizedIteratorRange(iter, result.iterator, ), is_ignored=True, ), )
                    else:
                        assert ignored_indentation_level != 0
                        ignored_indentation_level -= 1

                    iter = result.iterator.Clone()
                elif next_token_type == NormalizedIterator.TokenType.WhitespacePrefix:
                    iter.SkipWhitespacePrefix()
                elif next_token_type == NormalizedIterator.TokenType.Content:
                    num_data_items = len(data_items)
                    result = self._horizontal_whitespace_token.Match_(iter.Clone(), )
                    if result is not None:
                        data_items.InsertBack_(TokenLexResultData(self._horizontal_whitespace_token, result, NormalizedIteratorRange(iter, result.iterator, ), is_ignored=True, ), )
                        iter = result.iterator.Clone()

                    at_beginning_of_line = iter.OffsetProper() == iter.LineInfoProper().content_start
                    result = self.comment_token.Match_(iter.Clone(), )
                    if result is not None:
                        data_items.InsertBack_(TokenLexResultData(self.comment_token, result, NormalizedIteratorRange(iter, result.iterator, ), is_ignored=True, ), )
                        eat_next_newline = at_beginning_of_line
                        iter = result.iterator.Clone()

                    if len(data_items) != num_data_items:
                        continue

                    break
                elif next_token_type == NormalizedIterator.TokenType.WhitespaceSuffix:
                    result = self._horizontal_whitespace_token.Match_(iter.Clone(), )
                    assert result is not None
                    data_items.InsertBack_(TokenLexResultData(self._horizontal_whitespace_token, result, NormalizedIteratorRange(iter, result.iterator, ), is_ignored=True, ), )
                    iter = result.iterator.Clone()
                elif next_token_type == NormalizedIterator.TokenType.EndOfLine:
                    if (not eat_next_newline and not at_content_start and ignore_whitespace_ctr == 0 and ignored_indentation_level is None):
                        break

                    result = self._newline_token.Match_(iter.Clone(), )
                    assert result is not None
                    data_items.InsertBack_(TokenLexResultData(self._newline_token, result, NormalizedIteratorRange(iter, result.iterator, ), is_ignored=True, ), )
                    iter = result.iterator.Clone()
                    eat_next_newline = False
                else:
                    assert False, next_token_type

            return data_items

        success = False
        data_items = List()
        ignored_data_items = None # as <List<LexResultData, >{min_length'=1, } | None>
        preserved_ignore_whitespace_ctr = None # as <UIntArch | None>
        prev_token_was_pop_control = False
        prev_line = None
        for phrase_index in Range(starting_phrase_index, len(self._phrases), ):
            phrase = self._phrases[phrase_index]
            if ignored_data_items is None:
                ignored_data_items = ExtractIgnoredDataItems()
                if iter.AtEnd():
                    break


            if phrase.__class__.__name__ == "TokenPhrase" and phrase.token.is_control_token:
                if phrase.token.__class__.__name__ == "PushIgnoreWhitespaceControlToken":
                    ignore_whitespace_ctr += 1
                elif phrase.token.__class__.__name__ == "PopIgnoreWhitespaceControlToken":
                    assert ignore_whitespace_ctr != 0
                    ignore_whitespace_ctr -= 1
                elif phrase.token.__class__.__name__ == "PushPreserveWhitespaceControlToken":
                    assert preserved_ignore_whitespace_ctr is None
                    preserved_ignore_whitespace_ctr = ignore_whitespace_ctr
                    ignore_whitespace_ctr = 0
                elif phrase.token.__class__.__name__ == "PopPreserveWhitespaceControlToken":
                    assert preserved_ignore_whitespace_ctr is not None
                    ignore_whitespace_ctr = preserved_ignore_whitespace_ctr
                    preserved_ignore_whitespace_ctr = None
                else:
                    assert False, phrase.token

                prev_token_was_pop_control = phrase.token.opening_token is not None
                if not prev_token_was_pop_control and ignored_data_items is not None:
                    if ignored_data_items:
                        iter = ignored_data_items[0].range.begin.Clone()

                    ignored_data_items = None

                continue

            result = phrase.Lex(unique_id + ("{} [{}]".format(self.Name, phrase_index, ), ), iter, observer, ignore_whitespace=ignore_whitespace_ctr != 0, )
            if result is None:
                return None

            if (result.success and result.range.begin != result.range.end and ignored_data_items is not None):
                if ignored_data_items:
                    data_items += ignored_data_items

                ignored_data_items = None

            if result.data is not None:
                data_items.InsertBack_(result.data, )

            iter = result.range.end.Clone()
            if not result.Success:
                success = False
                break

            success = True
            if result.range.begin != result.range.end and iter.LineProper() != prev_line:
                prev_line = iter.LineProper()
                print("Parsing: Ln {}".format(prev_line))
        if ignored_data_items is not None:
            if ignored_data_items:
                if ignored_data_items[-1].range.end.AtEnd():
                    assert False, "BugBug"

                if prev_token_was_pop_control:
                    data_items += ignored_data_items
                    iter = ignored_data_items[-1].range.end.Clone()
                else:
                    iter = ignored_data_items[0].range.begin.Clone()


            ignored_data_items = None

        return LexResult(success, NormalizedIteratorRange(original_iter, iter, ), PhraseLexResultData(self, Phrase.PhraseContainerLexResultData(data_items, is_complete=True, ), unique_id, ), )

    # Return Type: String
    @staticmethod
    def _CreateDefaultName(phrases, ):
        return "[{}]".format(", ".join(phrase.Name for phrase in phrases, ), )
