# ----------------------------------------------------------------------
# |
# |  TokenPhrase.TheLanguage
# |
# |  David Brownell <db@DavidBrownell.com>
# |      2021-12-05 19:01:52
# |
# ----------------------------------------------------------------------
# |
# |  Copyright David Brownell 2021
# |  Distributed under the Boost Software License, Version 1.0. See
# |  accompanying file LICENSE_1_0.txt or copy at
# |  http://www.boost.org/LICENSE_1_0.txt.
# |
# ----------------------------------------------------------------------
<<<
Contains the `TokenPhrase` object.
>>>

from ..Components.Phrase import Phrase, NormalizedIterator

# BugBUg: Verify that these imports are correct after refactor is complete
from ..Components.Token import (
    DedentToken,
    IndentToken,
    NewlineToken,
    RegexToken,
    Token,
)


# ----------------------------------------------------------------------
[PrivateCtor]
public mutable class TokenPhrase
    extends Phrase
:
    # ----------------------------------------------------------------------
    # |
    # |  Public Data
    # |
    # ----------------------------------------------------------------------
    public Token val token

    python_hack: @property
    python_hack: def Token(self): return self.token

    # ----------------------------------------------------------------------
    # |
    # |  Public Methods
    # |
    # ----------------------------------------------------------------------
    public static TokenPhrase Create(
        Token val token,
        (String | None) var name = None,
    ):
        if name is None:
            name = token.name

        return TokenPhrase(move name, token)

    # ----------------------------------------------------------------------
    # TODO: public abstract val (LexResult | None) val Lex(
    public override immutable (LexResult | None) val Lex(
        any:
            UniqueId val unique_id,
            NormalizedIterator var iter,
            Observer ref observer,
        key:
            Bool val ignore_whitespace = False,
    ):
        python_hack: LexResult = Phrase.LexResult
        python_hack: NormalizedIteratorRange = Phrase.NormalizedIteratorRange
        python_hack: PhraseLexResultData = Phrase.PhraseLexResultData
        python_hack: TokenLexResultData = Phrase.TokenLexResultData

        # TODO: var result = None as (Token.MatchResult | None)
        var result = None as (Token_MatchResult | None)

        observer.StartPhrase(unique_id, self)
        # TODO: exit: observer.EndPhrase(unique_id, self, result is not None)

        result = self.token.Match?(iter.Clone())
        if result is None:
            python_hack: observer.EndPhrase(unique_id, self, result is not None)

            # TODO: with iter as val:
            # TODO:     return LexResult(
            # TODO:         False,
            # TODO:         NormalizedIteratorRange(iter, iter),
            # TODO:         # TODO: It seems that the next field really should be None, however it can't be during this code transition
            # TODO:         PhraseLexResultData(self, None, None),
            # TODO:     )

            val iter_clone = iter.Clone()

            return LexResult(
                False,
                NormalizedIteratorRange(iter_clone, iter_clone),
                # TODO: It seems that the next field really should be None, however it can't be during this code transition
                PhraseLexResultData(self, None, None),
            )

        val iter_range = NormalizedIteratorRange(
            # TODO: iter as val,
            iter,
            result.iterator,
        )

        val data = PhraseLexResultData(
            self,
            TokenLexResultData(
                self.token,
                # TODO (cast problems): result as val,
                result,
                iter_range,
                is_ignored=self.token.is_always_ignored,
            ),
            unique_id,
        )

        if self.token is IndentToken:
            observer.OnPushScopeProxy(data, iter_range)
        elif self.token is DedentToken:
            observer.OnPopScopeProxy(data, iter_range)
        elif not observer.OnInternalPhraseProxy(data, iter_range):
            return None

        python_hack: observer.EndPhrase(unique_id, self, result is not None)

        return LexResult(True, iter_range, data)

    # ----------------------------------------------------------------------
    # |
    # |  Private Methods
    # |
    # ----------------------------------------------------------------------
    private override mutable Bool val _PopulateRecursiveImpl(Phrase val new_phrase):
        # Nothing downstream will be populated
        return False

    python_hack: from typing import List, Optional, Tuple
    python_hack: _indent_token                           = IndentToken.Create()
    python_hack: _dedent_token                           = DedentToken.Create()
    python_hack: _newline_token                          = NewlineToken.Create()
    python_hack: # TODO: Caching opportunity
    python_hack: @classmethod
    python_hack: def ExtractPotentialCommentsOrWhitespace(
    python_hack:     cls,
    python_hack:     comment_token: RegexToken,
    python_hack:     normalized_iter: Phrase.NormalizedIterator,
    python_hack:     ignored_indentation_level: Optional[int],
    python_hack:     *,
    python_hack:     ignore_whitespace: bool,
    python_hack:     next_phrase_is_indent: bool,
    python_hack:     next_phrase_is_dedent: bool,
    python_hack: ) -> Optional[
    python_hack:     Tuple[
    python_hack:         List[Phrase.TokenLexResultData],
    python_hack:         Phrase.NormalizedIterator,
    python_hack:         Optional[int],                              # Updated 'ignored_indentation_level' value
    python_hack:     ]
    python_hack: ]:
    python_hack:     # Note that this should only be invoked on a phrase associated with a token that represents
    python_hack:     # a comment.
    python_hack:     data_items: List[Phrase.TokenLexResultData] = []
    python_hack:     while not normalized_iter.AtEnd():
    python_hack:         if ignore_whitespace:
    python_hack:             process_whitespace = True
    python_hack:             consume_indent = True
    python_hack:             consume_dedent = ignored_indentation_level != 0
    python_hack:         elif normalized_iter.Offset == 0:
    python_hack:             # If we are at the beginning of the content, consume any leading
    python_hack:             # newlines. We don't need to worry about a content that starts with
    python_hack:             # newline-comment-newline as the comment extract code will handle
    python_hack:             # the newline(s) that appears after the comment.
    python_hack:             process_whitespace = True
    python_hack:             consume_indent = False
    python_hack:             consume_dedent = False
    python_hack:         else:
    python_hack:             process_whitespace = False
    python_hack:             consume_indent = False
    python_hack:             consume_dedent = False
    python_hack:         if process_whitespace:
    python_hack:             data_item = cls.ExtractPotentialWhitespaceToken(
    python_hack:                 normalized_iter,
    python_hack:                 consume_indent=consume_indent,
    python_hack:                 consume_dedent=consume_dedent,
    python_hack:                 consume_newline=True,
    python_hack:             )
    python_hack:             if data_item is not None:
    python_hack:                 if ignored_indentation_level is not None:
    python_hack:                     if isinstance(data_item.Token, IndentToken):
    python_hack:                         ignored_indentation_level += 1
    python_hack:                     elif isinstance(data_item.Token, DedentToken):
    python_hack:                         assert ignored_indentation_level
    python_hack:                         ignored_indentation_level -= 1
    python_hack:                 data_items.append(data_item)
    python_hack:                 normalized_iter = data_item.IterEnd.Clone()
    python_hack:                 continue
    python_hack:         # Check for comments
    python_hack:         potential_comment_items = cls._ExtractPotentialCommentTokens(
    python_hack:             comment_token,
    python_hack:             normalized_iter,
    python_hack:             next_phrase_is_indent=next_phrase_is_indent,
    python_hack:             next_phrase_is_dedent=next_phrase_is_dedent,
    python_hack:         )
    python_hack:         if potential_comment_items is not None:
    python_hack:             potential_comment_items, normalized_iter = potential_comment_items
    python_hack:             data_items += potential_comment_items
    python_hack:             continue
    python_hack:         # If here, we didn't find anything
    python_hack:         break
    python_hack:     if not data_items:
    python_hack:         return None
    python_hack:     return (data_items, normalized_iter, ignored_indentation_level)
    python_hack: # ----------------------------------------------------------------------
    python_hack: # TODO: Caching opportunity
    python_hack: @staticmethod
    python_hack: def ExtractPotentialWhitespace(
    python_hack:     normalized_iter: Phrase.NormalizedIterator,
    python_hack: ) -> Optional[Tuple[int, int]]:
    python_hack:     """Consumes any whitespace located at the current offset"""
    python_hack:     if normalized_iter.AtEnd():
    python_hack:         return None
    python_hack:     next_token_type = normalized_iter.GetNextTokenType()
    python_hack:     if next_token_type == Phrase.NormalizedIterator.TokenType.WhitespacePrefix:
    python_hack:         normalized_iter.SkipWhitespacePrefix()
    python_hack:     elif (
    python_hack:         next_token_type == Phrase.NormalizedIterator.TokenType.Content
    python_hack:         or next_token_type == Phrase.NormalizedIterator.TokenType.WhitespaceSuffix
    python_hack:     ):
    python_hack:         start = normalized_iter.Offset
    python_hack:         while (
    python_hack:             normalized_iter.Offset < normalized_iter.LineInfo.OffsetEnd
    python_hack:             and normalized_iter.Content[normalized_iter.Offset].isspace()
    python_hack:             and normalized_iter.Content[normalized_iter.Offset] != "\n"
    python_hack:         ):
    python_hack:             normalized_iter.Advance(1)
    python_hack:         if normalized_iter.Offset != start:
    python_hack:             return start, normalized_iter.Offset
    python_hack:     return None
    python_hack: # ----------------------------------------------------------------------
    python_hack: # TODO: Caching opportunity
    python_hack: @classmethod
    python_hack: def _ExtractPotentialCommentTokens(
    python_hack:     cls,
    python_hack:     comment_token: RegexToken,
    python_hack:     normalized_iter: Phrase.NormalizedIterator,
    python_hack:     *,
    python_hack:     next_phrase_is_indent: bool,
    python_hack:     next_phrase_is_dedent: bool,
    python_hack: ) -> Optional[Tuple[List[Phrase.TokenLexResultData], Phrase.NormalizedIterator]]:
    python_hack:     """Eats any comment (stand-along or trailing) when requested"""
    python_hack:     normalized_iter = normalized_iter.Clone()
    python_hack:     next_token_type = normalized_iter.GetNextTokenType()
    python_hack:     if next_token_type == Phrase.NormalizedIterator.TokenType.Indent:
    python_hack:         # There are 2 scenarios to consider here:
    python_hack:         #
    python_hack:         #     1) The indent that we are looking at is because the comment itself
    python_hack:         #        is indented. Example:
    python_hack:         #
    python_hack:         #           Line 1: Int value = 1       # The first line of the comment
    python_hack:         #           Line 2:                     # The second line of the comment
    python_hack:         #           Line 3: value += 1
    python_hack:         #
    python_hack:         #        In this scenario, Line 2 starts with an indent and Line 3 starts with
    python_hack:         #        a dedent. We want to consume and ignore both the indent and dedent.
    python_hack:         #
    python_hack:         #     2) The indent is a meaningful part of the statement. Example:
    python_hack:         #
    python_hack:         #           Line 1: class Foo():
    python_hack:         #           Line 2:     # A comment
    python_hack:         #           Line 3:     Int value = 1
    python_hack:         #
    python_hack:         #        In this scenario, Line 2 is part of the class declaration and we want
    python_hack:         #        the indent to be officially consumed before we process the comment.
    python_hack:         if next_phrase_is_indent:
    python_hack:             # We are in scenario #2
    python_hack:             return None
    python_hack:         # If here, we are in scenario #1
    python_hack:         at_beginning_of_line = True
    python_hack:         normalized_iter_offset = normalized_iter.Offset
    python_hack:         normalized_iter.SkipWhitespacePrefix()
    python_hack:         assert normalized_iter.Offset > normalized_iter_offset
    python_hack:         potential_whitespace = (normalized_iter_offset, normalized_iter.Offset)
    python_hack:     elif next_token_type == Phrase.NormalizedIterator.TokenType.WhitespacePrefix:
    python_hack:         at_beginning_of_line = True
    python_hack:         normalized_iter.SkipWhitespacePrefix()
    python_hack:         potential_whitespace = None
    python_hack:     else:
    python_hack:         at_beginning_of_line = (
    python_hack:             normalized_iter.Offset == normalized_iter.LineInfo.OffsetStart
    python_hack:             or normalized_iter.Offset == normalized_iter.LineInfo.ContentStart
    python_hack:         )
    python_hack:         potential_whitespace = cls.ExtractPotentialWhitespace(normalized_iter)
    python_hack:     normalized_iter_begin = normalized_iter.Clone()
    python_hack:     result = comment_token.Match_(normalized_iter)
    python_hack:     if result is None:
    python_hack:         return None
    python_hack:     results = [
    python_hack:         # pylint: disable=too-many-function-args
    python_hack:         Phrase.TokenLexResultData(
    python_hack:             comment_token,
    python_hack:             result,
    python_hack:             Phrase.NormalizedIteratorRange(normalized_iter_begin, normalized_iter),
    python_hack:             is_ignored=True,
    python_hack:         ),
    python_hack:     ]
    python_hack:     if at_beginning_of_line:
    python_hack:         # Add additional content to consume the entire line...
    python_hack:         # Capture the trailing newline
    python_hack:         result = cls.ExtractPotentialWhitespaceToken(
    python_hack:             results[-1].IterEnd,
    python_hack:             consume_indent=False,
    python_hack:             consume_dedent=False,
    python_hack:             consume_newline=True,
    python_hack:         )
    python_hack:         assert result
    python_hack:         results.append(result)
    python_hack:         # Consume potential dedents, but don't return it with the results (as we absorbed the
    python_hack:         # corresponding indent above when we skipped the prefix). We need to do this as every
    python_hack:         # dedent must have a matching indent token.
    python_hack:         if (
    python_hack:             results[0].Whitespace is not None
    python_hack:             and results[-1].IterEnd.GetNextTokenType() == Phrase.NormalizedIterator.TokenType.Dedent
    python_hack:             and not next_phrase_is_dedent
    python_hack:         ):
    python_hack:             result = cls.ExtractPotentialWhitespaceToken(
    python_hack:                 results[-1].IterEnd,
    python_hack:                 consume_indent=False,
    python_hack:                 consume_dedent=True,
    python_hack:                 consume_newline=False,
    python_hack:             )
    python_hack:             assert result
    python_hack:             # Even though we aren't sending the dedent token, we need to make sure that the returned
    python_hack:             # iterator is updated to the new position. Comments are special beasts.
    python_hack:             return (results, result.IterEnd)
    python_hack:     return (results, results[-1].IterEnd)
    python_hack: # ----------------------------------------------------------------------
    python_hack: # TODO: Caching opportunity
    python_hack: @classmethod
    python_hack: def ExtractPotentialWhitespaceToken(
    python_hack:     cls,
    python_hack:     normalized_iter: Phrase.NormalizedIterator,
    python_hack:     *,
    python_hack:     consume_indent: bool,
    python_hack:     consume_dedent: bool,
    python_hack:     consume_newline: bool,
    python_hack: ) -> Optional[Phrase.TokenLexResultData]:
    python_hack:     """Eats any whitespace token when requested"""
    python_hack:     next_token_type = normalized_iter.GetNextTokenType()
    python_hack:     if next_token_type == Phrase.NormalizedIterator.TokenType.Indent:
    python_hack:         if not consume_indent:
    python_hack:             return None
    python_hack:         normalized_iter_begin = normalized_iter.Clone()
    python_hack:         result = cls._indent_token.Match_(normalized_iter)
    python_hack:         assert result
    python_hack:         # pylint: disable=too-many-function-args
    python_hack:         return Phrase.TokenLexResultData(
    python_hack:             cls._indent_token,
    python_hack:             result,
    python_hack:             Phrase.NormalizedIteratorRange(normalized_iter_begin, normalized_iter),
    python_hack:             is_ignored=True,
    python_hack:         )
    python_hack:     elif next_token_type == Phrase.NormalizedIterator.TokenType.Dedent:
    python_hack:         if not consume_dedent:
    python_hack:             return None
    python_hack:         normalized_iter_begin = normalized_iter.Clone()
    python_hack:         result = cls._dedent_token.Match_(normalized_iter)
    python_hack:         assert result
    python_hack:         # pylint: disable=too-many-function-args
    python_hack:         return Phrase.TokenLexResultData(
    python_hack:             cls._dedent_token,
    python_hack:             result,
    python_hack:             Phrase.NormalizedIteratorRange(normalized_iter_begin, normalized_iter),
    python_hack:             is_ignored=True,
    python_hack:         )
    python_hack:     # Consume potential newlines
    python_hack:     if consume_newline:
    python_hack:         normalized_iter = normalized_iter.Clone()
    python_hack:         normalized_iter_begin = normalized_iter.Clone()
    python_hack:         if next_token_type == Phrase.NormalizedIterator.TokenType.EndOfLine:
    python_hack:             potential_whitespace = None
    python_hack:         else:
    python_hack:             potential_whitespace = cls.ExtractPotentialWhitespace(normalized_iter)
    python_hack:         if normalized_iter.GetNextTokenType() == Phrase.NormalizedIterator.TokenType.EndOfLine:
    python_hack:             result = cls._newline_token.Match_(normalized_iter)
    python_hack:             assert result
    python_hack:             # pylint: disable=too-many-function-args
    python_hack:             return Phrase.TokenLexResultData(
    python_hack:                 cls._newline_token,
    python_hack:                 result,
    python_hack:                 Phrase.NormalizedIteratorRange(normalized_iter_begin, normalized_iter),
    python_hack:                 is_ignored=True,
    python_hack:             )
    python_hack:     return None
